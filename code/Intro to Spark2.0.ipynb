{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark\n",
    "\n",
    "## Resilient Distributed Datasets\n",
    "* Patitioned\n",
    "* Immutable\n",
    "* Resilient: Know their lineage\n",
    "### Creation:\n",
    "* Reading a file\n",
    "* Tranforming another RDD\n",
    "### Processes:\n",
    "#### Transformation: \n",
    "* Creates another RDD\n",
    "* Stored in form of directed acyclic graph\n",
    "#### Action: \n",
    "* Lazy Approach\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrames\n",
    "\n",
    "Similar to pandas df\n",
    "\n",
    "*Why DF instead of RDD?\n",
    "Usually easier to work on structured data instead of RDDs*\n",
    "\n",
    "Also, since DF is build on RDDs:\n",
    "* Immutable\n",
    "* Distributed\n",
    "* Resilient\n",
    "\n",
    "Similarity to Relational DB:\n",
    "* Has named columns\n",
    "* Conceptually equal\n",
    "\n",
    "**No type safety at compile time**: Be careful\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Architecture](https://pluralsight.imgix.net/course-images/spark-2-getting-started-v1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Master Node runs the driver program, which is its own separate process\n",
    "\n",
    "1.x sparkContext is the gateway\n",
    "\n",
    "In Spark 2.x we use the SparkSession, which encapsulates SparkContext and others like HiveContext and SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Driver\n",
    "* Runs on the Master Node\n",
    "* Separate process\n",
    "* Responsible for launching tasks\n",
    "* Sets up SparkSession env\n",
    "* Schedules jobs\n",
    "* Allocates resources\n",
    "* Runs many services to manage cluster\n",
    "* Communicates with worker nodes, retrieves and compiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Application\n",
    "* Uses SparkSession as entry point\n",
    "    * Encapsulates the SparkContext, SQLContext, HiveContext\n",
    "* Serves as a bridge to the underlying env\n",
    "* Creates a DAG pf operations to perform on data\n",
    "* Internally creates Stages (physical exec plan)\n",
    "* Each Stage split into operations on data partitions called Tasks - logical operation on RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.x vs 2.x\n",
    "\n",
    "1.x\n",
    "* Resembles optimizations on a traditional R-DB\n",
    "* Volcano iterator model\n",
    "* Missed several code/compiler operations\n",
    "\n",
    "2.x\n",
    "* 2nd gen Tungsten engine: Improved performance\n",
    "* Elimination of virtual function calls\n",
    "* Store data in registers, not RAM/cache\n",
    "* Compiler loop unrolling, pipelining\n",
    "\n",
    "Spark 2.x is 10x faster than 1.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Manager\n",
    "\n",
    "* Orchestrates execution: can be plugged into Spark\n",
    "    * Hadoop's YARN\n",
    "    * Apache Mesos\n",
    "    * Spark Standalone\n",
    "* Distributes task among worker node and coordinate executions between them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workers\n",
    "\n",
    "* Compute nodes within cluster\n",
    "* Runs Spark application code\n",
    "* Allocated resources by master\n",
    "* Use executors to run jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data = sc.parallelize([1, \"Nissan Versa\", 12])\n",
    "simple_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_data.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
